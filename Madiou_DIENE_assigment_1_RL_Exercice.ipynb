{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYW2YAMZOX4-"
   },
   "source": [
    "# Reinforcement Learning in Finite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFu6XLHrtIa6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8K4vLMPTSxmn"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
    "!cd mvarl_hands_on && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gridworld in /home/aimsgh/.local/lib/python3.7/site-packages (0.1)\r\n",
      "Requirement already satisfied: arcade in /home/aimsgh/.local/lib/python3.7/site-packages (from gridworld) (2.3.6)\r\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from gridworld) (1.16.2)\r\n",
      "Requirement already satisfied: pytiled-parser in /home/aimsgh/.local/lib/python3.7/site-packages (from arcade->gridworld) (0.9.2)\r\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from arcade->gridworld) (5.4.1)\r\n",
      "Requirement already satisfied: pyglet in /home/aimsgh/.local/lib/python3.7/site-packages (from arcade->gridworld) (1.5.0)\r\n",
      "Requirement already satisfied: pyglet-ffmpeg2 in /home/aimsgh/.local/lib/python3.7/site-packages (from arcade->gridworld) (0.1.17)\r\n",
      "Requirement already satisfied: attrs in /usr/lib/python3/dist-packages (from pytiled-parser->arcade->gridworld) (18.2.0)\r\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from pyglet->arcade->gridworld) (0.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNsuKZxeOX4_"
   },
   "source": [
    "## MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNlEnGsYOX5A"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './mvarl_hands_on/utils')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "# from gridworld import GridWorldWithPits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting finite_env\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: 404 Client Error: Not Found for url: https://pypi.org/simple/finite-env/\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install finite_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puk7xD1EBDEu"
   },
   "source": [
    "## Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezz31WR2uJFX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finite_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c706c7b3542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinite_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFiniteEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: You do not need to read this Class. It only redefines the clearning robots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'finite_env'"
     ]
    }
   ],
   "source": [
    "from finite_env import FiniteEnv\n",
    "import numpy as np\n",
    "\n",
    "# Note: You do not need to read this Class. It only redefines the clearning robots\n",
    "\n",
    "\n",
    "class RobotEnv(FiniteEnv):\n",
    "    \"\"\"\n",
    "    Enviroment with 2 states and 3 actions\n",
    "    Args:\n",
    "        gamma (float): discount factor\n",
    "        seed    (int): Random number generator seed\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.5, seed=42):\n",
    "        # Set seed\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "        # Transition probabilities\n",
    "        # shape (Ns, Na, Ns)\n",
    "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
    "\n",
    "        Ns = 2\n",
    "        Na = 3\n",
    "        \n",
    "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
    "        P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
    "        print(P)\n",
    "        print(P.shape)\n",
    "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
    "\n",
    "        self.state_decoder  = {0: \"A\", 1: \"B\"}\n",
    "        self.action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
    "        \n",
    "        # Initialize base class\n",
    "        states = np.arange(Ns).tolist()\n",
    "        action_sets = [np.arange(Na).tolist()]*Ns\n",
    "        print(states)\n",
    "        print(action_sets)\n",
    "        super().__init__(states, action_sets, P, gamma)\n",
    "\n",
    "    def reward_func(self, state, action, *_):\n",
    "        return self._R[state, action]\n",
    "\n",
    "    def reset(self, s=0):\n",
    "        self.state = s\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.sample_transition(self.state, action)\n",
    "        reward = self.reward_func(self.state, action, next_state)\n",
    "        done = False\n",
    "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
    "            self.state_decoder[state],\n",
    "            self.action_decoder[action],\n",
    "            self.state_decoder[next_state],\n",
    "            reward )}\n",
    "        self.state = next_state\n",
    "\n",
    "        observation = next_state\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def sample_transition(self, s, a):\n",
    "        prob = self.P[s,a,:]\n",
    "        s_ = self.RS.choice(self.states, p = prob)\n",
    "        return s_\n",
    "\n",
    "    def render_policy(self, policy):\n",
    "        if len(np.array(policy).shape) > 1:\n",
    "            policy = densify_policy(policy)\n",
    "\n",
    "        txt = \"\"\n",
    "        for i, a in enumerate(policy):\n",
    "            txt += \"In state {} perform {}\\n\".format(self.state_decoder[i], self.action_decoder[a])\n",
    "        return txt[:-1]\n",
    "\n",
    "    @property\n",
    "    def R(self):\n",
    "        return self._R\n",
    "  \n",
    "    env = RobotEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hdzoq4dLOX5C"
   },
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVHOHJJeOX5C"
   },
   "outputs": [],
   "source": [
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"R has shape: \", env.R.shape)  \n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Usefult methods\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=0, a=1): \", env.reward_func(0,1))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", policy)\n",
    "print(env.render_policy(policy))\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(4):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward, \"   --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YM7lLbRA68A"
   },
   "source": [
    "## Useful functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sekFIFmm6V71"
   },
   "outputs": [],
   "source": [
    "# This functions ains at alternating between sparse and dense policy:\n",
    "# sparse policy allows to perform matrix completion pi \\in R^{SxA}. It is denoted pi in the code Ex: [[1, 0, 0], [0, 1, 0]])\n",
    "# dense policy is a determinist policy whose value are the indexed actions. It is the argmax of pi. It is denoted dpi in the code. Ex [0, 1] \n",
    "\n",
    "def sparsify_policy(policy, Na):\n",
    "    ### Turn a dense policy into a sparse one.\n",
    "    #  Ex: [0, 1], Na=3  -> [[1, 0, 0], [0, 1, 0]]\n",
    "    ###\n",
    "\n",
    "    Ns = len(policy)\n",
    "    sparse_policy = np.zeros(shape=(Ns, Na))\n",
    "    for i, a in enumerate(policy):\n",
    "        sparse_policy[i,a]=1\n",
    "        return sparse_policy\n",
    "\n",
    "def densify_policy(policy):\n",
    "  ### Turn a sparse determinist policy into a dense one.\n",
    "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
    "  ###\n",
    "  return np.array(policy).argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4dyTP0kkAXKo"
   },
   "source": [
    "## Exercice : Policy Evaluation\n",
    "1. Evaluate the policy by solving the linear system\n",
    "2. Evaluate the policy through recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1T0QX3tA2RRU"
   },
   "outputs": [],
   "source": [
    "# Policy evaluation (exact)\n",
    "\n",
    "# Retrieve the environment MDP\n",
    "P = env.P\n",
    "R = env.R\n",
    "gamma = env.gamma\n",
    "\n",
    "\n",
    "# Policy to evaluate\n",
    "# State A: Search\n",
    "# State B: Wait\n",
    "dpi = np.array([1, 0])\n",
    "pi = sparsify_policy(dpi, Na=env.Na)\n",
    "print(\"## old pi:\")\n",
    "print(pi)\n",
    "print(env.render_policy(pi))\n",
    "\n",
    "# states = [0, 1]\n",
    "# actions = [0, 1, 2]\n",
    "# Ppi = np.zeros([2,2])\n",
    "# for s in states:\n",
    "#   for s_next in states:\n",
    "#     Ppi[s, s_next] = np.sum(pi[s] * P[s, :, s_next]) \n",
    "\n",
    "# Compute the dynamics given the policy\n",
    "Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n",
    "Rpi = np.sum(R * pi, axis=1) #Note, we assume that R(s,a) does not depend of the next state\n",
    "\n",
    "# Evaluate the policy\n",
    "Vpi = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
    "print(\"## Vpi: \")\n",
    "print(Vpi)\n",
    "\n",
    "\n",
    "# # Compute the Q values\n",
    "Qpi = R + gamma * P.dot(Vpi)\n",
    "\n",
    "# print(\"## Qpi:\")\n",
    "print(\"## Qpi: \")\n",
    "print('**')\n",
    "print(Qpi)\n",
    "\n",
    "\n",
    "# # What is the next policy if we perform one step of policy improvment ?\n",
    "dpi_new = Qpi.argmax(axis=1)\n",
    "\n",
    "# print(\"## new pi:\")\n",
    "print(\"## pi: \")\n",
    "print(sparsify_policy(dpi_new, Na=env.Na))\n",
    "print(env.render_policy(dpi_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZb0Piis2YX4"
   },
   "outputs": [],
   "source": [
    "# Policy evaluation (recursive)\n",
    "\n",
    "# Retrieve the environment MDP\n",
    "P = env.P\n",
    "R = env.R\n",
    "gamma = env.gamma\n",
    "\n",
    "# Policy to evaluate\n",
    "dpi = np.array([1, 0])\n",
    "pi = sparsify_policy(dpi, Na=env.Na)\n",
    "\n",
    "# Compute the dynamics given the policy\n",
    "Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n",
    "Rpi = np.sum(R * pi, axis=1) \n",
    "\n",
    "# Stopping criterion -> maximum number of steps\n",
    "epsilon = 1e-3\n",
    "r_max = np.max(R)\n",
    "max_k = int(np.log(r_max/epsilon)/np.log(1/gamma))+1\n",
    "\n",
    "v = np.array([-1, 1])\n",
    "print(\"0:\", v)\n",
    "for k in range(max_k):\n",
    "    v = Rpi + gamma*Ppi.dot(v)\n",
    "    print(\"{}:\".format(k), v)\n",
    "\n",
    "print()\n",
    "\n",
    "# Stopping criterion -> compute the infinite norm\n",
    "v_new = np.array([-1, 1])\n",
    "v_old = np.array([2*epsilon, 0])\n",
    "k = 1\n",
    "print(\"0:\", v_new)\n",
    "while np.absolute(v_new-v_old).max() > epsilon:\n",
    "    v_old = v_new\n",
    "    v_new = Rpi + gamma*Ppi.dot(v_new)\n",
    "    print(\"{}:\".format(k), v_new)\n",
    "    k += 1\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0nl8OfLwpdC"
   },
   "source": [
    "## New Environment!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDjMyANRs8Z6"
   },
   "outputs": [],
   "source": [
    "# New Maze environment\n",
    "# s: start\n",
    "# g: goal\n",
    "# x: negative reward state\n",
    "\n",
    "grid1 = [\n",
    "    ['', '', '', 'g'],\n",
    "    ['', 'x', '', ''],\n",
    "    ['s', '', '', '']\n",
    "]\n",
    "grid1_MAP = [\n",
    "    \"+-------+\",\n",
    "    \"| : : :G|\",\n",
    "    \"| :x: : |\",\n",
    "    \"|S: : : |\",\n",
    "    \"+-------+\",\n",
    "]\n",
    "\n",
    "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRUnx8EstjFY"
   },
   "outputs": [],
   "source": [
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"R has shape: \", env.R.shape)  \n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Usefult methods\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "dpi = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", dpi)\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(4):\n",
    "    action = dpi[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward) \n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsXJD1qF74Iy"
   },
   "outputs": [],
   "source": [
    "dpi = np.random.randint(env.Na, size = (env.Ns,))\n",
    "env.render_policy(dpi)\n",
    "\n",
    "state = env.reset()\n",
    "env.render()\n",
    "for i in range(5):\n",
    "    action = dpi[state]\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ugaKsHsOX5G"
   },
   "source": [
    "## Exercice: Value Iteration\n",
    "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
    "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition \n",
    "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
    "4. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvlitQzHD7Fc"
   },
   "outputs": [],
   "source": [
    "# useful function\n",
    "def plot_infnorm(lst, star, name=\"V\"):\n",
    "  \n",
    "    lst = np.array(lst)\n",
    "    star = np.array(star)\n",
    "\n",
    "     # Compute inf norm\n",
    "    diff = np.absolute(lst - star).max(axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(diff)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(\"||{} - {}*||_inf\".format(name, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKkDqSyBOX5G"
   },
   "outputs": [],
   "source": [
    "# Compute Value Iteration\n",
    "\n",
    "# Retrieve the environment MDP\n",
    "P = env.P\n",
    "R = env.R\n",
    "gamma = env.gamma\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Prepare v, and storage\n",
    "v = np.zeros(env.Ns)\n",
    "v_all = []\n",
    "pi_all = []\n",
    "\n",
    "# iterate over the value\n",
    "while True:\n",
    "    \n",
    "  \n",
    "  # Compute v_k\n",
    "    v, v_old = np.max(R + gamma * P.dot(v), axis=1), v\n",
    "    v_all.append(v)\n",
    "\n",
    "  # Esimate Intermediate policy\n",
    "    q = R + gamma * P.dot(v)\n",
    "    dpi = q.argmax(axis=1)\n",
    "    print(dpi)\n",
    "\n",
    "    pi_all.append(dpi)\n",
    "\n",
    "    # stopping criterion \n",
    "    if np.absolute(v-v_old).max() < epsilon: \n",
    "    break\n",
    "\n",
    "# Plot optimal policy\n",
    "env.render_policy(dpi)\n",
    "\n",
    "# You need to evaluate V* here from your optimal policy\n",
    "# You have no guarantee that the last v-value is converge to the actual Vpi\n",
    "pi = sparsify_policy(dpi, Na=env.Na)\n",
    "print(pi)\n",
    "Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n",
    "Rpi = np.sum(R * pi, axis=1)\n",
    "\n",
    "v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
    "print(\"v (last):\", v_all[-1])\n",
    "print(\"v (star):\", v_star)\n",
    "\n",
    "# The difference is not exactly epsilon. We here we are comparing V*-V while the algorithm we are comparing V_new - V_old\n",
    "print(\"epsilon: \", epsilon)\n",
    "print(\"|v - v_star|_inf: \", np.absolute(v_all[-1]-v_star).max())\n",
    "\n",
    "# The value function converge towards v* iteration after iteration\n",
    "plot_infnorm(v_all, v_star, name=\"V\")\n",
    "\n",
    "# If we estimate the intermediate policies, we observe that the optimal policy is obtained earlier in the process.\n",
    "# However, the value function has not yet converge. Thus ares still unaware that it is the optimal policty at iteration 4.\n",
    "plot_infnorm(pi_all, dpi, name=\"Pi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDT4iyJnAPIE"
   },
   "source": [
    "## Exercice: Policy Iteration (Homework)\n",
    "1. Implement Policy iteration!\n",
    "2. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
    "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpBCGnqNE7l1"
   },
   "outputs": [],
   "source": [
    "# Retrieve the environment MDP\n",
    "P = env.P\n",
    "R = env.R\n",
    "gamma = env.gamma\n",
    "epsilon = 5e-2\n",
    "import copy\n",
    "\n",
    "v_all = []\n",
    "p_all = []\n",
    "\n",
    "#Initialize policy\n",
    "dpi = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
    "print(env.render_policy(dpi))\n",
    "\n",
    "def policy_eval(env, policy, gamma=env.gamma, eps = 0.05):\n",
    "    \n",
    "    Ppi = np.sum(P * np.expand_dims(policy, axis=-1), axis=1)\n",
    "    Rpi = np.sum(R * policy, axis=1)\n",
    "    v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
    "    return v_star\n",
    "\n",
    "def policy_inprovemnt(env, va_ameleorate):\n",
    "    q = R + gamma * P.dot(va_ameleorate)\n",
    "    dpi = q.argmax(axis=1)\n",
    "    return dpi\n",
    "\n",
    "def policy_iteration(env, gamma=env.gamma, eps = 0.05):\n",
    "    old_policy = np.zeros(env.Ns, dtype=np.int32)\n",
    "    while True:\n",
    "    v = policy_eval(env, sparsify_policy(old_policy, env.Na))\n",
    "    new_policy = policy_inprovemnt(env, v)\n",
    "    if (new_policy == old_policy).all():\n",
    "        break\n",
    "\n",
    "    old_policy = copy.copy(new_policy)\n",
    "    v_all.append(v)\n",
    "    p_all.append(new_policy)\n",
    "  return old_policy, v\n",
    "\n",
    "save_, v = policy_iteration(env)\n",
    "\n",
    "print(env.render_policy(save_))\n",
    "\n",
    "plot_infnorm(v_all, v, name=\"V\")\n",
    "\n",
    "\n",
    "plot_infnorm(pi_all, save_, name=\"Pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHrlauOwOX5L"
   },
   "source": [
    "## Exercice: Q learning\n",
    "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
    "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
    "\n",
    "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
    "  - Plot the error in Q-functions over iterations\n",
    "  - Plot the cumulative sum of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCxJruBNOX5M"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Q-Learning\n",
    "# ---------------------------\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    Q learning with epsilon-greedy exploration\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.Q = np.zeros((env.Ns, env.Na))\n",
    "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
    "    \n",
    "    def sample_action(self, state, greedy=False):\n",
    "        epsilon = self.epsilon / max(1, math.sqrt(self.Nsa[state].min()))\n",
    "        \n",
    "        # explore\n",
    "        if not greedy and np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(self.env.actions)\n",
    "\n",
    "        # exploit\n",
    "        else:\n",
    "            action = self.Q[state, :].argmax()\n",
    "            return self.env.actions[action]\n",
    "        \n",
    "    \n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        alpha = self.learning_rate / max(1, math.sqrt(self.Nsa[state, action]))\n",
    "        \n",
    "        if not done:\n",
    "          max_q = self.Q[next_state, :].max()\n",
    "        else:\n",
    "          max_q = 0.  # We do not bootstrap further\n",
    "        \n",
    "        q = self.Q[state, action]\n",
    "        increment = (reward + self.gamma*max_q) - q\n",
    "\n",
    "        # Update\n",
    "        self.Q[state, action] = self.Q[state, action] + alpha*increment\n",
    "        self.Nsa[state, action] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsmojgubLVL9"
   },
   "outputs": [],
   "source": [
    "q_learning = QLearning(env, gamma=env.gamma, learning_rate=1, epsilon=0.6, min_epsilon=0.1)\n",
    "\n",
    "# Define storage and variable\n",
    "q_all = []\n",
    "r_all = []\n",
    "pi_all = []\n",
    "max_steps = int(5e4)\n",
    "\n",
    "\n",
    "# main algorithmic loop\n",
    "state = env.reset()\n",
    "for t in range(max_steps):\n",
    "    \n",
    "    \n",
    "  # Sample the action\n",
    "    action = q_learning.sample_action(state, greedy=False)\n",
    "  \n",
    "  # Sample the environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # Update q-function\n",
    "    q_learning.update(state=state, action=action, next_state=next_state, reward=reward, done=done)\n",
    "\n",
    "    # Store information \n",
    "    r_all.append(reward)\n",
    "    q_all.append(q_learning.Q)\n",
    "    pi_all.append(q_learning.Q.argmax(axis=1))\n",
    "\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "dpi = q_learning.Q.argmax(axis=1)\n",
    "print(env.render_policy(dpi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysm4Cq_pxj5T"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "for i in range(5):\n",
    "    action = q_learning.sample_action(state, greedy=True)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QyNzQpLIpAv"
   },
   "outputs": [],
   "source": [
    "# Does the Q-value in the goal-state make sense?\n",
    "print(q_learning.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bi2j8vI4It8h"
   },
   "outputs": [],
   "source": [
    "plot_infnorm(pi_all, pi_all[-1], name=\"Pi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xViQ6kpzJfw9"
   },
   "outputs": [],
   "source": [
    "pi = sparsify_policy(pi_all[-1], Na=env.Na)\n",
    "Ppi = np.sum(env.P * np.expand_dims(pi, axis=-1), axis=1)\n",
    "Rpi = np.sum(env.R * pi, axis=1)\n",
    "\n",
    "v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
    "\n",
    "v_all = []\n",
    "for dpi_prev, q_prev in zip(pi_all, q_all):\n",
    "    pi_prev = sparsify_policy(dpi_prev, Na=env.Na)\n",
    "    v_prev = np.sum(pi_prev*q_prev, axis=1)\n",
    "    v_all.append(v_prev)\n",
    "\n",
    "\n",
    "# You may notive that some states have a high errors (especially the one with the negative rewards)\n",
    "# Indeed, as there are less and less explored, the Q-values in \"bad\" states are not updated anymore as they are avoided by the agent.\n",
    "plot_infnorm(v_all, v_star, name=\"V\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59xsIjpIOX5P"
   },
   "source": [
    "## Exercice: SARSA (Homework)\n",
    "SARSA is another control algorithm. While Qlearning is off-policy, SARSA is on-policy.\n",
    "\n",
    "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
    "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gySnHtrsOX5Q"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# SARSA\n",
    "# ---------------------------\n",
    "class SARSA:\n",
    "    \"\"\"\n",
    "    SARSA with deacreasing epsilon for exploration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma, min_epsilon, learning_rate, epsilon):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((env.Ns, env.Na))\n",
    "        self.Nsa = np.ones((env.Ns, env.Na))\n",
    "    \n",
    "    def sample_action(self, state, greedy=False):\n",
    "        epsilon = min(\n",
    "            self.epsilon / math.sqrt(self.Nsa[state].min()),\n",
    "            self.min_epsilon)\n",
    "        \n",
    "        # explore\n",
    "        if not greedy and np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(self.env.actions)\n",
    "\n",
    "        # exploit\n",
    "        else:\n",
    "            action = self.Q[state, :].argmax()\n",
    "        return self.env.actions[action]\n",
    "\n",
    "\n",
    "    def update(self, state, action, next_state, next_action, reward):\n",
    "        alpha = self.learning_rate / math.sqrt(self.Nsa[state, action])\n",
    "        \n",
    "        if not done:\n",
    "            max_q = self.Q[next_state, next_action]\n",
    "        else:\n",
    "            max_q = 0.  # We do not bootstrap further\n",
    "        \n",
    "        q = self.Q[state, action]\n",
    "        increment = (reward + self.gamma*max_q) - q\n",
    "\n",
    "        # Update\n",
    "        self.Q[state, action] = self.Q[state, action] + alpha*increment\n",
    "        self.Nsa[state, action] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Be0d4nOBOX5R"
   },
   "outputs": [],
   "source": [
    "\n",
    "sarsa = SARSA(env, gamma=env.gamma, min_epsilon = 0.1, learning_rate=1., epsilon=1.)\n",
    "\n",
    "max_steps = int(5e4)\n",
    "\n",
    "\n",
    "# main algorithmic loop\n",
    "state = env.reset()\n",
    "action = sarsa.sample_action(state, greedy=False)\n",
    "\n",
    "for t in range(max_steps):\n",
    "    next_state, reward, done,_ = env.step(action)\n",
    "    \n",
    "  # Sample the action\n",
    "    next_action = sarsa.sample_action(next_state, greedy=False)\n",
    "  \n",
    "  # Sample the environment\n",
    "  \n",
    "  # Update q-function\n",
    "sarsa.update(state=state, action=action, next_state=next_state, next_action = next_action, reward=reward)\n",
    "\n",
    "  # Store information \n",
    "\n",
    "state = next_state\n",
    "action = next_action\n",
    " if done:\n",
    "    state = env.reset()\n",
    "    action = sarsa.sample_action(state, greedy=False)\n",
    "\n",
    "dpi = sarsa.Q.argmax(axis=1)\n",
    "print(env.render_policy(dpi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZpijtxq-Y6N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Madiou_DIENE_assigment_1_RL.Exercice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
